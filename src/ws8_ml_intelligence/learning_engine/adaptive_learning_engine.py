"""
Adaptive Learning Engine - ML Intelligence System

This module implements the adaptive learning system that learns from historical
patterns, week types, and system performance to provide insights and improve
understanding of market conditions and system behavior.

IMPORTANT: This system provides ADVISORY insights only. All wealth management
decisions remain 100% rules-based per Constitution v1.3.

Mission: "Autopilot for Wealth.....Engineered for compounding income and corpus"
"""

from typing import Dict, Any, Optional, List, Tuple
from decimal import Decimal
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import json

from src.ws1_rules_engine.constitution.week_classification import WeekType, WeekPerformance
from src.ws1_rules_engine.audit import AuditTrailManager

logger = logging.getLogger(__name__)


class LearningMode(Enum):
    """Learning modes for the adaptive engine."""
    WEEK_TYPE_LEARNING = "week_type_learning"
    PERFORMANCE_PATTERN_LEARNING = "performance_pattern_learning"
    RISK_PATTERN_LEARNING = "risk_pattern_learning"
    MARKET_CONDITION_LEARNING = "market_condition_learning"
    PROTOCOL_EFFECTIVENESS_LEARNING = "protocol_effectiveness_learning"


@dataclass
class LearningData:
    """Data structure for learning inputs."""
    timestamp: datetime
    week_type: WeekType
    week_performance: WeekPerformance
    market_data: Dict[str, float]
    performance_metrics: Dict[str, float]
    risk_metrics: Dict[str, float]
    protocol_level: str
    system_actions: List[str]
    outcomes: Dict[str, float]


@dataclass
class LearningInsight:
    """Insight generated by the learning engine."""
    insight_type: str
    confidence: float
    description: str
    supporting_data: Dict[str, Any]
    recommendations: List[str]
    generated_at: datetime


@dataclass
class ModelPerformance:
    """Model performance metrics."""
    model_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    r2_score: float
    mse: float
    training_samples: int
    last_updated: datetime


class AdaptiveLearningEngine:
    """
    Adaptive learning system that learns from historical patterns and provides
    insights to enhance understanding of the wealth management system.
    
    ADVISORY ONLY: Provides insights and recommendations but does not make
    any wealth management decisions. All decisions remain rules-based.
    """
    
    def __init__(self,
                 audit_manager: AuditTrailManager,
                 model_storage_path: str = "/tmp/ml_models"):
        """Initialize the adaptive learning engine."""
        self.audit_manager = audit_manager
        self.model_storage_path = model_storage_path
        
        # Learning models
        self.models: Dict[LearningMode, Any] = {}
        self.scalers: Dict[LearningMode, StandardScaler] = {}
        self.model_performance: Dict[LearningMode, ModelPerformance] = {}
        
        # Learning data storage
        self.learning_data: List[LearningData] = []
        self.insights_cache: List[LearningInsight] = []
        
        # Feature engineering
        self.feature_columns = self._define_feature_columns()
        
        # Initialize models
        self._initialize_models()
        
        logger.info("AdaptiveLearningEngine initialized successfully")
    
    def _define_feature_columns(self) -> Dict[LearningMode, List[str]]:
        """Define feature columns for each learning mode."""
        return {
            LearningMode.WEEK_TYPE_LEARNING: [
                'market_volatility', 'market_return', 'vix_level', 'vix_change',
                'volume_ratio', 'sector_rotation', 'economic_surprise_index',
                'earnings_density', 'options_expiration', 'holiday_proximity'
            ],
            LearningMode.PERFORMANCE_PATTERN_LEARNING: [
                'week_type_encoded', 'market_volatility', 'market_return',
                'portfolio_beta', 'portfolio_alpha', 'position_count',
                'capital_utilization', 'protocol_level_encoded', 'vix_level'
            ],
            LearningMode.RISK_PATTERN_LEARNING: [
                'portfolio_volatility', 'market_correlation', 'concentration_risk',
                'liquidity_risk', 'delta_exposure', 'gamma_exposure',
                'theta_exposure', 'vega_exposure', 'protocol_level_encoded'
            ],
            LearningMode.MARKET_CONDITION_LEARNING: [
                'market_return', 'market_volatility', 'vix_level', 'vix_change',
                'yield_curve_slope', 'credit_spreads', 'dollar_strength',
                'commodity_performance', 'sector_dispersion'
            ],
            LearningMode.PROTOCOL_EFFECTIVENESS_LEARNING: [
                'protocol_level_encoded', 'market_stress_level', 'portfolio_drawdown',
                'volatility_regime', 'correlation_regime', 'liquidity_conditions',
                'time_in_protocol', 'previous_protocol_success'
            ]
        }
    
    def _initialize_models(self):
        """Initialize ML models for each learning mode."""
        for mode in LearningMode:
            # Use Random Forest for most learning tasks
            if mode in [LearningMode.WEEK_TYPE_LEARNING, LearningMode.MARKET_CONDITION_LEARNING]:
                # Classification tasks
                from sklearn.ensemble import RandomForestClassifier
                self.models[mode] = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1
                )
            else:
                # Regression tasks
                self.models[mode] = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1
                )
            
            # Initialize scaler
            self.scalers[mode] = StandardScaler()
    
    async def add_learning_data(self, data: LearningData):
        """Add new learning data to the system."""
        try:
            self.learning_data.append(data)
            
            # Trigger learning if we have enough data
            if len(self.learning_data) % 100 == 0:  # Retrain every 100 samples
                await self._trigger_learning()
            
            # Log the data addition
            await self.audit_manager.log_event(
                event_type="ML_LEARNING_DATA_ADDED",
                details={
                    'timestamp': data.timestamp.isoformat(),
                    'week_type': data.week_type.value,
                    'week_performance': data.week_performance.value,
                    'protocol_level': data.protocol_level
                },
                metadata={'learning_data_count': len(self.learning_data)}
            )
            
        except Exception as e:
            logger.error(f"Error adding learning data: {str(e)}")
    
    async def _trigger_learning(self):
        """Trigger learning process for all modes."""
        try:
            for mode in LearningMode:
                await self._train_model(mode)
            
            # Generate new insights
            await self._generate_insights()
            
            logger.info("Learning process completed successfully")
            
        except Exception as e:
            logger.error(f"Error in learning process: {str(e)}")
    
    async def _train_model(self, mode: LearningMode):
        """Train a specific model."""
        try:
            if len(self.learning_data) < 50:  # Need minimum data
                return
            
            # Prepare training data
            X, y = self._prepare_training_data(mode)
            
            if len(X) == 0:
                return
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Scale features
            X_train_scaled = self.scalers[mode].fit_transform(X_train)
            X_test_scaled = self.scalers[mode].transform(X_test)
            
            # Train model
            self.models[mode].fit(X_train_scaled, y_train)
            
            # Evaluate model
            y_pred = self.models[mode].predict(X_test_scaled)
            
            # Calculate performance metrics
            if mode in [LearningMode.WEEK_TYPE_LEARNING, LearningMode.MARKET_CONDITION_LEARNING]:
                # Classification metrics
                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                r2 = 0.0
                mse = 0.0
            else:
                # Regression metrics
                accuracy = 0.0
                precision = 0.0
                recall = 0.0
                f1 = 0.0
                r2 = r2_score(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
            
            # Store performance
            self.model_performance[mode] = ModelPerformance(
                model_name=f"{mode.value}_model",
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1,
                r2_score=r2,
                mse=mse,
                training_samples=len(X_train),
                last_updated=datetime.utcnow()
            )
            
            # Save model
            await self._save_model(mode)
            
            logger.info(f"Model {mode.value} trained successfully with {len(X_train)} samples")
            
        except Exception as e:
            logger.error(f"Error training model {mode.value}: {str(e)}")
    
    def _prepare_training_data(self, mode: LearningMode) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare training data for a specific learning mode."""
        try:
            feature_cols = self.feature_columns[mode]
            
            # Convert learning data to DataFrame
            data_dicts = []
            for data in self.learning_data:
                row = {}
                
                # Add market data features
                for col in feature_cols:
                    if col in data.market_data:
                        row[col] = data.market_data[col]
                    elif col in data.performance_metrics:
                        row[col] = data.performance_metrics[col]
                    elif col in data.risk_metrics:
                        row[col] = data.risk_metrics[col]
                    elif col == 'week_type_encoded':
                        row[col] = self._encode_week_type(data.week_type)
                    elif col == 'protocol_level_encoded':
                        row[col] = self._encode_protocol_level(data.protocol_level)
                    else:
                        row[col] = 0.0  # Default value
                
                # Add target variable based on mode
                if mode == LearningMode.WEEK_TYPE_LEARNING:
                    row['target'] = self._encode_week_type(data.week_type)
                elif mode == LearningMode.PERFORMANCE_PATTERN_LEARNING:
                    row['target'] = data.outcomes.get('weekly_return', 0.0)
                elif mode == LearningMode.RISK_PATTERN_LEARNING:
                    row['target'] = data.outcomes.get('realized_volatility', 0.0)
                elif mode == LearningMode.MARKET_CONDITION_LEARNING:
                    row['target'] = self._encode_market_condition(data.market_data)
                elif mode == LearningMode.PROTOCOL_EFFECTIVENESS_LEARNING:
                    row['target'] = data.outcomes.get('protocol_success_score', 0.0)
                
                data_dicts.append(row)
            
            if not data_dicts:
                return np.array([]), np.array([])
            
            df = pd.DataFrame(data_dicts)
            
            # Separate features and target
            X = df[feature_cols].fillna(0).values
            y = df['target'].fillna(0).values
            
            return X, y
            
        except Exception as e:
            logger.error(f"Error preparing training data for {mode.value}: {str(e)}")
            return np.array([]), np.array([])
    
    def _encode_week_type(self, week_type: WeekType) -> int:
        """Encode week type as integer."""
        encoding = {
            WeekType.NORMAL: 0,
            WeekType.VOLATILE: 1,
            WeekType.TRENDING: 2,
            WeekType.CONSOLIDATION: 3,
            WeekType.EARNINGS: 4,
            WeekType.EXPIRATION: 5,
            WeekType.HOLIDAY: 6,
            WeekType.EVENT_DRIVEN: 7,
            WeekType.CRISIS: 8,
            WeekType.RECOVERY: 9
        }
        return encoding.get(week_type, 0)
    
    def _encode_protocol_level(self, protocol_level: str) -> int:
        """Encode protocol level as integer."""
        encoding = {'L0': 0, 'L1': 1, 'L2': 2, 'L3': 3}
        return encoding.get(protocol_level, 0)
    
    def _encode_market_condition(self, market_data: Dict[str, float]) -> int:
        """Encode market condition based on market data."""
        volatility = market_data.get('market_volatility', 0.15)
        returns = market_data.get('market_return', 0.0)
        
        if volatility > 0.25:
            return 2  # High volatility
        elif abs(returns) > 0.02:
            return 1  # Trending
        else:
            return 0  # Normal
    
    async def _save_model(self, mode: LearningMode):
        """Save trained model to disk."""
        try:
            import os
            os.makedirs(self.model_storage_path, exist_ok=True)
            
            model_path = f"{self.model_storage_path}/{mode.value}_model.joblib"
            scaler_path = f"{self.model_storage_path}/{mode.value}_scaler.joblib"
            
            joblib.dump(self.models[mode], model_path)
            joblib.dump(self.scalers[mode], scaler_path)
            
        except Exception as e:
            logger.error(f"Error saving model {mode.value}: {str(e)}")
    
    async def _generate_insights(self):
        """Generate insights based on learned patterns."""
        try:
            insights = []
            
            # Generate insights for each learning mode
            for mode in LearningMode:
                if mode in self.model_performance:
                    performance = self.model_performance[mode]
                    
                    if performance.accuracy > 0.7 or performance.r2_score > 0.5:
                        insight = await self._generate_mode_insight(mode, performance)
                        if insight:
                            insights.append(insight)
            
            # Store insights
            self.insights_cache.extend(insights)
            
            # Keep only recent insights (last 100)
            self.insights_cache = self.insights_cache[-100:]
            
            logger.info(f"Generated {len(insights)} new insights")
            
        except Exception as e:
            logger.error(f"Error generating insights: {str(e)}")
    
    async def _generate_mode_insight(self,
                                   mode: LearningMode,
                                   performance: ModelPerformance) -> Optional[LearningInsight]:
        """Generate insight for a specific learning mode."""
        try:
            if mode == LearningMode.WEEK_TYPE_LEARNING:
                return LearningInsight(
                    insight_type="Week Type Pattern",
                    confidence=performance.accuracy,
                    description=f"The system has learned to predict week types with {performance.accuracy:.1%} accuracy. This helps anticipate market conditions and adjust expectations accordingly.",
                    supporting_data={
                        'model_accuracy': performance.accuracy,
                        'training_samples': performance.training_samples,
                        'feature_importance': self._get_feature_importance(mode)
                    },
                    recommendations=[
                        "Use week type predictions to set appropriate performance expectations",
                        "Consider week type patterns when evaluating system performance"
                    ],
                    generated_at=datetime.utcnow()
                )
            
            elif mode == LearningMode.PERFORMANCE_PATTERN_LEARNING:
                return LearningInsight(
                    insight_type="Performance Pattern",
                    confidence=performance.r2_score,
                    description=f"Performance patterns show R² of {performance.r2_score:.3f}, indicating {performance.r2_score:.1%} of performance variation is explained by learned patterns.",
                    supporting_data={
                        'r2_score': performance.r2_score,
                        'mse': performance.mse,
                        'training_samples': performance.training_samples,
                        'feature_importance': self._get_feature_importance(mode)
                    },
                    recommendations=[
                        "Monitor performance patterns for early warning signals",
                        "Use pattern insights to understand performance drivers"
                    ],
                    generated_at=datetime.utcnow()
                )
            
            # Add more insight types as needed
            
        except Exception as e:
            logger.error(f"Error generating insight for {mode.value}: {str(e)}")
        
        return None
    
    def _get_feature_importance(self, mode: LearningMode) -> Dict[str, float]:
        """Get feature importance for a model."""
        try:
            if hasattr(self.models[mode], 'feature_importances_'):
                feature_cols = self.feature_columns[mode]
                importances = self.models[mode].feature_importances_
                
                return dict(zip(feature_cols, importances.tolist()))
        except Exception as e:
            logger.error(f"Error getting feature importance for {mode.value}: {str(e)}")
        
        return {}
    
    async def get_insights(self, 
                          insight_type: Optional[str] = None,
                          min_confidence: float = 0.5) -> List[LearningInsight]:
        """Get insights from the learning engine."""
        insights = self.insights_cache
        
        if insight_type:
            insights = [i for i in insights if i.insight_type == insight_type]
        
        if min_confidence > 0:
            insights = [i for i in insights if i.confidence >= min_confidence]
        
        return sorted(insights, key=lambda x: x.generated_at, reverse=True)
    
    async def predict_week_type(self, market_data: Dict[str, float]) -> Tuple[WeekType, float]:
        """Predict week type based on market data."""
        try:
            mode = LearningMode.WEEK_TYPE_LEARNING
            
            if mode not in self.models or mode not in self.scalers:
                return WeekType.NORMAL, 0.0
            
            # Prepare features
            feature_cols = self.feature_columns[mode]
            features = []
            
            for col in feature_cols:
                features.append(market_data.get(col, 0.0))
            
            # Scale and predict
            features_scaled = self.scalers[mode].transform([features])
            prediction = self.models[mode].predict(features_scaled)[0]
            
            # Get prediction probability if available
            confidence = 0.5
            if hasattr(self.models[mode], 'predict_proba'):
                probabilities = self.models[mode].predict_proba(features_scaled)[0]
                confidence = max(probabilities)
            
            # Decode prediction
            week_types = list(WeekType)
            predicted_week_type = week_types[int(prediction) % len(week_types)]
            
            return predicted_week_type, confidence
            
        except Exception as e:
            logger.error(f"Error predicting week type: {str(e)}")
            return WeekType.NORMAL, 0.0
    
    async def predict_performance(self,
                                market_data: Dict[str, float],
                                portfolio_data: Dict[str, float]) -> Tuple[float, float]:
        """Predict expected performance based on current conditions."""
        try:
            mode = LearningMode.PERFORMANCE_PATTERN_LEARNING
            
            if mode not in self.models or mode not in self.scalers:
                return 0.0, 0.0
            
            # Prepare features
            feature_cols = self.feature_columns[mode]
            features = []
            
            for col in feature_cols:
                if col in market_data:
                    features.append(market_data[col])
                elif col in portfolio_data:
                    features.append(portfolio_data[col])
                else:
                    features.append(0.0)
            
            # Scale and predict
            features_scaled = self.scalers[mode].transform([features])
            prediction = self.models[mode].predict(features_scaled)[0]
            
            # Estimate confidence based on model performance
            confidence = self.model_performance.get(mode, ModelPerformance(
                model_name="", accuracy=0, precision=0, recall=0, f1_score=0,
                r2_score=0.5, mse=0, training_samples=0, last_updated=datetime.utcnow()
            )).r2_score
            
            return float(prediction), confidence
            
        except Exception as e:
            logger.error(f"Error predicting performance: {str(e)}")
            return 0.0, 0.0
    
    async def get_model_performance(self) -> Dict[str, ModelPerformance]:
        """Get performance metrics for all models."""
        return {mode.value: perf for mode, perf in self.model_performance.items()}
    
    async def retrain_all_models(self):
        """Retrain all models with current data."""
        await self._trigger_learning()
    
    async def clear_learning_data(self, keep_recent_days: int = 30):
        """Clear old learning data, keeping recent data."""
        cutoff_date = datetime.utcnow() - timedelta(days=keep_recent_days)
        
        original_count = len(self.learning_data)
        self.learning_data = [
            data for data in self.learning_data
            if data.timestamp >= cutoff_date
        ]
        
        cleared_count = original_count - len(self.learning_data)
        logger.info(f"Cleared {cleared_count} old learning data entries, kept {len(self.learning_data)}")
        
        return cleared_count

